{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#General############################################\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import pprint\r\n",
    "from time import sleep\r\n",
    "from allfunctions import *\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "from matplotlib.pyplot import table\r\n",
    "\r\n",
    "#WebScraping########################################\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "\r\n",
    "#Spotify############################################\r\n",
    "import spotipy\r\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\r\n",
    "from Credentials import Client_ID, Client_Secret\r\n",
    "import json\r\n",
    "\r\n",
    "#Machine Learning###################################\r\n",
    "from sklearn import datasets \r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.cluster import KMeans\r\n",
    "from sklearn.metrics import silhouette_score\r\n",
    "import pickle\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective Day 1\n",
    "\n",
    "\n",
    "Create a function to scrape the Billboards 100 HOT songs and create local dataframe of songs with them including:\n",
    "\n",
    "* Song’s name\n",
    "\n",
    "* Song’s artist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective Day 2\n",
    "\n",
    "Create a second function to retrieve the audio features of a given song.\n",
    "\n",
    "Create a third function to update the internal database with the song features using the previous function and to add this information to the dataframe \n",
    "\n",
    "\n",
    "* Song’s url/ uri\n",
    "\n",
    "* Song’s audio_features\n",
    "\n",
    "\n",
    "Recommendation: create a function to extend the internal database with songs of your choice/playlist,.... The more songs you have in the database, the better. (you will see tomorrow why).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective Day 3\n",
    "\n",
    "* Fit the standard scaler to scale the audio features of each song using both dataframes.\n",
    "\n",
    "* Save the freshly trained standard scaler with pickle\n",
    "\n",
    "* Get the user’s song audio features\n",
    "\n",
    "* Apply the trained scaler to transform the user’s song audio features\n",
    "\n",
    "Create a function to fit  the K-Means clustering method using all the songs contained in the both datasets ( hot100 and spotify ).\n",
    "\n",
    "Remember to:\n",
    "- Do some research on the optimal K-value for the K-means\n",
    "- Save the final K-means model with pickle\n",
    "- Use the trained K-means model to predict the cluster of each song in the internal databases and add this information to the internal databases\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective Day 4\n",
    "\n",
    "Create a function to ask the user:\n",
    "\n",
    "- Song title\n",
    "- Song artist\n",
    "\n",
    "Create a function to search the user song in the Spotify API to get the audio features\n",
    "\n",
    "Create a function to retrieve the standard scaler previously saved and to scale the song’s audio features\n",
    "\n",
    "Create a function to retrieve the K-Means model and predicts to which cluster belongs the song.\n",
    "\n",
    "Create a function to retrieve one/more songs at random from the corresponding database which belongs to the same cluster as the user’s song and it’s not the same as the user song.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Webscraping Hot 100"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Link to Hot 100 Chart\r\n",
    "url = \"https://www.billboard.com/charts/hot-100\"\r\n",
    "\r\n",
    "#Setting up Spotify API\r\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=Client_ID, client_secret=Client_Secret)\r\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Importing url info\r\n",
    "response = requests.get(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Getting the html code for the website.\r\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#List of song titles from the Hot 100 chart\r\n",
    "top_100_song_titles = [tag.select(\"span.chart-element__information__song.text--truncate.color--primary\")[0].get_text() for tag in soup.select(\"span.chart-element__information\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#List of song artists from the Hot 100 chart\r\n",
    "top_100_song_artists = [elem.select(\"span.chart-element__information__artist.text--truncate.color--secondary\")[0].get_text() for elem in soup.select(\"span.chart-element__information\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating a dictionary with all the information\r\n",
    "top_100_artists_titles_dict = {\"artist_Names\":top_100_song_artists, \"song_Names\":top_100_song_titles} "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Converting the dictionary to a dataframe\r\n",
    "top_100_artists_titles_df = create_dataframe_from_dictionary(top_100_artists_titles_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Audio features from one song"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Getting json file with all information about song.\r\n",
    "song = get_all_information_about_song(\"Butter\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we get the audiofeatures of our chosen song."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "audio_features = sp.audio_features(song[\"tracks\"][\"items\"][0][\"uri\"])[0]\r\n",
    "#audio_features     #The audio-features of the song are shown as a dictionary."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "audio_features_new = { key: [audio_features[key]] for key in list(audio_features.keys()) }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dataframe with the audio features for our chosen song.\r\n",
    "pd.DataFrame(audio_features_new) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code"
   ],
   "metadata": {
    "cell_style": "center"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Top 100 Songs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting all the song names from our Hot Songs 100 and making a dataframe with uri. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Uri's of top 100 in a list.\r\n",
    "top_100_uris = []\r\n",
    "for i in top_100_song_titles:\r\n",
    "    top_100_uris.append(get_uri_of_song(i))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#List with song uri's converted to list  of dicts of the audiofeatures.\r\n",
    "top_100_audio_features = get_audio_features_from_list_of_uris(top_100_uris)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating a dictionary with all the audio features and an empty list, which will be filled with the values from all songs.\r\n",
    "top_100_dict_audio_features = create_dict_with_audiofeatures_for_all_songs(top_100_audio_features)\r\n",
    "df_top_100 = pd.DataFrame(top_100_dict_audio_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random playlists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the information from a random playlist."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#This playlist has around 5000 songs.\r\n",
    "playlist_id = \"4ehqP8QHaIPdHMsssoB4y2\" \r\n",
    "\r\n",
    "#This creates a json list with all the songs info in the playlist.\r\n",
    "playlist =  get_playlist_tracks(\"spotify\", playlist_id) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#The path to the uri for each song. \r\n",
    "#playlist[0][\"track\"][\"uri\"] \r\n",
    "get_uri_of_song(playlist[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This part creates three lists, one with the uri's the other with the artists names. The last one is a list of song titles\r\n",
    "\r\n",
    "playlist_uri = [] #Creating a list with all the uris to get the other information.\r\n",
    "playlist_artist_names = [] #Creating a list with all the artist names.\r\n",
    "playlist_song_names = []\r\n",
    "\r\n",
    "for i in range(len(playlist)):\r\n",
    "    playlist_uri.append(playlist[i][\"track\"][\"uri\"]) #The i is the name of the song in the playlist.\r\n",
    "    playlist_artist_names.append(playlist[i][\"track\"][\"artists\"][0][\"name\"])\r\n",
    "    playlist_song_names.append(playlist[i][\"track\"][\"name\"])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "playlist_audio_features = get_audio_features_from_list_of_uris(playlist_uri)"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "center"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating a list for each key in the audiofeatures. All keys \r\n",
    "playlist_dict_audiofeature = create_dict_with_audiofeatures_for_all_songs(playlist_audio_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dataframe column with playlist audio features.\r\n",
    "df_playlist = pd.DataFrame(playlist_dict_audiofeature)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Adding the artist names and song artists to dataframe.\r\n",
    "df_artist_names = pd.DataFrame(playlist_artist_names, columns=[\"Artist_Names\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dataframe column with song names.\r\n",
    "df_song_names = pd.DataFrame(playlist_song_names, columns=[\"Song_Names\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = pd.concat([df_song_names, df_artist_names], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.concat([x, df_playlist], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the Dataframe from any random playlist by id using the function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "playlist_id_list1 = [\"37i9dQZF1DXcBWIGoYBM5M\", \r\n",
    "                     \"37i9dQZEVXbMDoHDwVN2tF\", \r\n",
    "                     \"37i9dQZF1DX0XUsuxWHRQd\", \r\n",
    "                     \"37i9dQZF1DX10zKzsJ2jva\", \r\n",
    "                     \"37i9dQZF1DWY7IeIP1cdjF\", \r\n",
    "                     \"37i9dQZF1DWWMOmoXKqHTD\", \r\n",
    "                     \"37i9dQZF1DX4o1oenSJRJd\", \r\n",
    "                     \"37i9dQZF1DWXRqgorJj26U\", \r\n",
    "                     \"37i9dQZF1DX4UtSsGT1Sbe\", \r\n",
    "                     \"37i9dQZF1DX76Wlfdnj7AP\", \r\n",
    "                     \"37i9dQZF1DX4WYpdgoIcn6\"]\r\n",
    "playlist_id_list2 = [\"68wQGN02SQNnDEc58HJfvn\", \r\n",
    "                     \"1iKrRpeXJbcxIDoixPYHXj\", \r\n",
    "                     \"6MMT0aj5QfhbG3mmaMxgt8\", \r\n",
    "                     \"1XSkjkrJjTAqksykUWD32Y\", \r\n",
    "                     \"6ibR5GQ3F2xsLQ1X4h7MnT\", \r\n",
    "                     \"37i9dQZF1E4slNaEJk0JW0\", \r\n",
    "                     \"6sLDvdiSbl9PxuvF8FD72B\"] \r\n",
    "                     "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating first big dataframe\r\n",
    "df_mega1 = mega_dataframe_from_playlist_id_list(playlist_id_list1)   \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating second big dataframe\r\n",
    "df_mega2 = mega_dataframe_from_playlist_id_list(playlist_id_list2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Concatenating the two big dataframes.\r\n",
    "data = pd.concat([df_mega1, df_mega2])\r\n",
    "data_copy = data.copy()"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data2 = df_top_100\r\n",
    "data_copy2 = data2.copy()"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(data_copy.isnull())"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(data_copy2.isnull())"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data_copy.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_copy = data_copy.dropna()"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_copy2 = data_copy2.dropna()"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_copy.columns"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_copy2.columns"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical = df_copy"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical2 = df_copy2"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#numerical"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical.drop([\"Song_Names\", \"Artist_Names\", \"type\", \"id\", \"uri\", \"track_href\", \"analysis_url\", \"time_signature\"], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical2.drop([\"type\", \"id\", \"uri\", \"track_href\", \"analysis_url\", \"time_signature\"], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**top 100 and internal database together**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering the scaled df with K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kmeans = KMeans(n_clusters=8, random_state=1234)\r\n",
    "kmeans.fit(numerical_scaled_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "K = range(2, 21)\r\n",
    "inertia = []\r\n",
    "silhouette = []\r\n",
    "\r\n",
    "for k in K:\r\n",
    "    print(\"Training a K-Means model with {} neighbours! \".format(k))\r\n",
    "    print()\r\n",
    "    kmeans = KMeans(n_clusters=k,random_state=1234)\r\n",
    "    kmeans.fit(numerical_scaled_df)\r\n",
    "    inertia.append(kmeans.inertia_)\r\n",
    "    filename = \"../datasets/kmeans_\" + str(k) + \".pickle\"\r\n",
    "    with open(filename, \"wb\") as f:\r\n",
    "        pickle.dump(kmeans,f)\r\n",
    "    silhouette.append(silhouette_score(numerical_scaled_df, kmeans.predict(numerical_scaled_df)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\r\n",
    "scaler.fit(numerical)\r\n",
    "numerical_scaled = scaler.transform(numerical)\r\n",
    "numerical_scaled_df = pd.DataFrame(numerical_scaled, columns = numerical.columns)\r\n",
    "display(numerical_scaled_df.head())\r\n",
    "data_cluster = kmeans.predict(numerical_scaled)"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\r\n",
    "scaler.fit(numerical2)\r\n",
    "numerical_scaled2 = scaler.transform(numerical2)\r\n",
    "numerical_scaled_df2 = pd.DataFrame(numerical_scaled2, columns = numerical2.columns)\r\n",
    "display(numerical_scaled_df2.head())\r\n",
    "data2_cluster = kmeans.predict(numerical_scaled2)"
   ],
   "outputs": [],
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerical3 = pd.concat([numerical, numerical2])\r\n",
    "scaler = StandardScaler()\r\n",
    "scaler.fit(numerical3)\r\n",
    "numerical_scaled3 = scaler.transform(numerical3)\r\n",
    "numerical_scaled_df3 = pd.DataFrame(numerical_scaled3, columns = numerical3.columns)\r\n",
    "display(numerical_scaled_df3.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "K = range(2, 21)\r\n",
    "inertia = []\r\n",
    "silhouette = []\r\n",
    "\r\n",
    "for k in K:\r\n",
    "    print(\"Training a K-Means model with {} neighbours! \".format(k))\r\n",
    "    print()\r\n",
    "    kmeans = KMeans(n_clusters=k,random_state=1234)\r\n",
    "    kmeans.fit(numerical_scaled_df3)\r\n",
    "    inertia.append(kmeans.inertia_)\r\n",
    "    filename = \"../datasets2/kmeans_\" + str(k) + \".pickle\"\r\n",
    "    with open(filename, \"wb\") as f:\r\n",
    "        pickle.dump(kmeans,f)\r\n",
    "    silhouette.append(silhouette_score(numerical_scaled_df3, kmeans.predict(numerical_scaled_df3)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, silhouette, 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('silhouette score')\r\n",
    "plt.xticks(np.arange(min(K), max(K)+1, 1.0))\r\n",
    "plt.title('Silhouette Method showing the optimal k')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,8))\r\n",
    "plt.plot(K, inertia, 'bx-')\r\n",
    "plt.xlabel('k')\r\n",
    "plt.ylabel('inertia')\r\n",
    "plt.xticks(np.arange(min(K), max(K)+1, 1.0))\r\n",
    "plt.title('Elbow Method showing the optimal k')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objective 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the freshly trained standard scaler with pickle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"../datasets/scaler.pickle\", \"wb\") as f:\r\n",
    "    pickle.dump(scaler,f)\r\n",
    "\r\n",
    "with open(\"../datasets/kmeans_4.pickle\", \"wb\") as f:\r\n",
    "    pickle.dump(kmeans,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def load(filepath = \"../data/kmeans.pickle\"): \r\n",
    "    try: \r\n",
    "        with open(filename, \"rb\") as f: \r\n",
    "            return pickle.load(f) \r\n",
    "    except FileNotFoundError: \r\n",
    "        print(\"File not found!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler2 = load(\"../datasets/scaler.pickle\")\r\n",
    "scaler2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apply the trained scaler to transform the user’s song audio features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "song_features = user_song_audio_featuers(\"Garden\")\r\n",
    "#x = get_audio_feature_items_into_seperate_lists(song_features)\r\n",
    "\r\n",
    "#.drop([\"type\", \"id\", \"uri\", \"track_href\", \"analysis_url\", \"time_signature\"], axis=1, inplace=True)\r\n",
    "#= song_features.drop(numerical.drop([\"type\", \"id\", \"uri\", \"track_href\", \"analysis_url\", \"time_signature\"], axis=1, inplace=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "song_name = input(\"Please input the song name: \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "song_dict = {\"Song_Name\": [song_name], \"Artist_Name\": [get_artists_name_from_song(song_name)]}\r\n",
    "song_df = pd.DataFrame(song_dict)\r\n",
    "#song_df.columns\r\n",
    "#scaler = StandardScaler()\r\n",
    "kmeans = load(\"../Users/kubra/Desktop/Week3/Project-Week3-Spotify-ML/datasets/datasets/kmeans_4.pickle\")\r\n",
    "song_features_dict = { key: [value] for key, value in song_features[0][0].items() }\r\n",
    "song_features_df = pd.DataFrame(song_features_dict)\r\n",
    "#song_features_df\r\n",
    "song_df = pd.concat([song_df,song_features_df], axis= 1)\r\n",
    "#song_df.columns \r\n",
    "song_numerical = song_df[numerical3.columns]\r\n",
    "song_scaled_numpy = scaler.transform(song_numerical)\r\n",
    "song_scaled_df = pd.DataFrame(song_scaled_numpy, columns=numerical3.columns)\r\n",
    "user_cluster = kmeans.predict(song_scaled_df)[0]\r\n",
    "clusters = kmeans.predict(numerical_scaled3)\r\n",
    "# top100\r\n",
    "# spotify\r\n",
    "# clusters \r\n",
    "\r\n",
    "data2['Cluster'] = clusters[0:data2.shape[0]]\r\n",
    "data['Cluster']  = clusters[data2.shape[0]:]\r\n",
    "#print(user_cluster)\r\n",
    "#Search user song in hot100 dataframe which has the name data2. The search is base in song id\r\n",
    "if ( song_df['id'].values in data2['id'].values ): # The song is in the hot100\r\n",
    "    recomendation = data2[ data2['Cluster'] == user_cluster ].sample()\r\n",
    "    print(\"Maybe you will also like the song {} from artist {}\".format(recomendation['Song_Names'].values[0],recomendation['Artist_Names'].values[0]))\r\n",
    "else:\r\n",
    "    recomendation = data[ data['Cluster'] == user_cluster ].sample()\r\n",
    "    print(\"Try this one. Maybe you will like the song {} from artist {}\".format(recomendation['Song_Names'].values[0],recomendation['Artist_Names'].values[0]))\r\n",
    "\r\n",
    "  \r\n",
    "\r\n",
    "#song_scaled = scaler.transform(song_features)\r\n",
    "#song_cluster = kmeans.predict(song_scaled)\r\n",
    "#song_final_df = song_df\r\n",
    "#song_final_df[\"Cluster\"] = song_cluster"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05b3b62c7789db94d00c942f2499dd9a2876ea971fd9b7cf25c50892de6c631c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}